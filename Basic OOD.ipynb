{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Requirements\n",
    " * numpy\n",
    " * torch\n",
    " * matplotlib\n",
    " * torchvision - for datasets\n",
    " * scipy for loading the datasets\n",
    " \n",
    " For setting up an environment please follow the instructions in https://github.com/AbinavRavi/OOD-detection/blob/master/README.md \n",
    " After setting up the environment please run the cells one by one. The notebook should run without a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision import datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The In -distrbution dataset that we will use in this demonstration is MNIST and the Out of distribution dataset is SVHN. \n",
    "\n",
    "Reasons for choosing SVHN as Out of Distribution dataset\n",
    "1. Similar size\n",
    "2. Different intensity\n",
    "\n",
    "In a production pipeline most images undergo same pre-processing and hence we don't need to choose the dataset during inference. Since this is a toy example we select an appropriate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.ToTensor()\n",
    "path = './IDdata/'\n",
    "if(os.path.exists(path) == False):\n",
    "    train_data = datasets.MNIST(root='IDdata', train=True,download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='IDdata',train=False,download=True,transform=transform)\n",
    "else:\n",
    "    train_data = datasets.MNIST(root='IDdata', train=True,download=False, transform=transform)\n",
    "    test_data = datasets.MNIST(root='IDdata',train=False,download=False,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26421880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to OODdata/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26427392it [00:02, 9074639.47it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OODdata/FashionMNIST/raw/train-images-idx3-ubyte.gz to OODdata/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 478273.11it/s]\n",
      "  1%|          | 40960/4422102 [00:00<00:11, 395464.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to OODdata/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting OODdata/FashionMNIST/raw/train-labels-idx1-ubyte.gz to OODdata/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to OODdata/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4423680it [00:00, 6927045.91it/s]                             \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OODdata/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to OODdata/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to OODdata/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 125166.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OODdata/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to OODdata/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transforms = T.Compose([T.ToTensor(), T.Resize(28)])\n",
    "out_path = './OODdata/'\n",
    "if(os.path.exists(out_path) == False):\n",
    "    ood_data = datasets.FashionMNIST(root='OODdata', train=True,download=True, transform=transform)\n",
    "else:\n",
    "    ood_data = datasets.FashionMNIST(root='OODdata',train=True,download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size,num_workers=num_workers)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,batch_size=batch_size,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
